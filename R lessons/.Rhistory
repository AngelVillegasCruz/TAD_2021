clean2$year[clean2$year==96]<-1996
clean2$year[clean2$year==97]<-1997
clean2$year[clean2$year==98]<-1998
clean2$year[clean2$year==99]<-1999
clean2$year[clean2$year==0]<-2000
clean2$year[clean2$year==1]<-2001
clean2$year[clean2$year==2]<-2002
clean2$year[clean2$year==3]<-2003
clean2$year[clean2$year==4]<-2004
clean2$year[clean2$year==5]<-2005
clean2$year[clean2$year==6]<-2006
clean2$year[clean2$year==7]<-2007
clean2$year[clean2$year==8]<-2008
##take a very small sample--can't run readability on the whole thing
sample<-sample(seq(1,length(clean2$texts)), 10000, replace = F  )
sample_data<-clean2[sample,]
congress_year <- sample_data$year
congress_stat <- textstat_readability(sample_data$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
congress_fre_df <- data.frame("year" = congress_year, "congress_stat" = congress_stat$Flesch)
congress_fre_sent <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanSentenceLength)
congress_fre_word <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanWordSyllables)
##melt together for plotting--Figure 3
require(reshape2)
df_US<-melt(list(SOTU=SOTU_fre_df, SCOTUS=balanced_scotus_fre_df, Congress = congress_fre_df,
ExecOrders = eo_fre_df), id.vars="year")
require(ggplot2)
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(-20, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Flesch Reading Ease Score") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1995, y = 45, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1800, y = 20, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 50, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1970, y = 0, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 5
df_US_sent<-melt(list(SOTU=SOTU_fre_sent, SCOTUS=balanced_scotus_fre_sent, Congress = congress_fre_sent,
ExecOrders = eo_fre_sent), id.vars="year")
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US_sent,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(15, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Sentence Length") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1960, y = 17, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1910, y = 25, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 30, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1960, y = 50, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 7
df_US_word<-melt(list(SOTU=SOTU_fre_word, SCOTUS=balanced_scotus_fre_word, Congress = congress_fre_word,
ExecOrders = eo_fre_word), id.vars="year")
p <- ggplot(data = df_US_word,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(1.4, 1.85)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Number of Syllables per Word") + theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "SOTU", x = 1800, y = 1.73, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 1.47, size = 6, colour = "black") +
annotate("text", label = "Con. Speech", x = 1998, y = 1.61, size = 6, colour = "black")+
annotate("text", label = "Executive Orders", x = 1950, y = 1.82, size = 6, colour = "black")
print(p)
##Figures 4, 6 and 8: Comparative data
data("data_corpus_SOTU")
SOTU_stat <- textstat_readability(data_corpus_SOTU, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
SOTU_year <- lubridate::year(docvars(data_corpus_SOTU, "Date"))
SOTU_fre_df <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$Flesch)
SOTU_fre_sent <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanSentenceLength)
SOTU_fre_word <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanWordSyllables)
########nobel
#load("data_text/NobelLitePresentations/data_corpus_nobel.rdata")
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
docvars(data_corpus_nobel, "Date")
docvars(data_corpus_nobel)
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "year"))
nobel_year <- (docvars(data_corpus_nobel, "year"))
install.packages("randomizr", repos="http://r.declaredesign.org")
library("randomizr", lib.loc="~/R/win-library/3.4")
remove.packages("randomizr", lib="~/R/win-library/3.4")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr", repos = "http://r.declaredesign.org")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr")
library("randomizr", lib.loc="~/R/win-library/3.4")
library(DeclareDesign)
library(quanteda)
library(sophistication)
data("data_corpus_Crimson")
library(dplyr)
require(stringr)
require(data.table)
##read in
load("C:/Users/kevin/Documents/GitHub/sophistication-papers/analysis_article/output/fitted_BT_model.Rdata")
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
data("data_corpus_SOTU")
library(quanteda)
install.packages("quanteda")
install.packages("quanteda")
library(quanteda)
data_corpus_SOTU
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
library(sophistication)
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
library(quanteda.quanteda)
library(quanteda.corpora)
install.packages("quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora", force = TRUE)
install.packages("digest")
library(readtext)
xx<-readtext(xx.txt)
xx<-readtext("xx.txt")
xx<-readtext(file ="xx.txt")
?readtext
xx<-readtext(file ="xx")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv", stringsAsFactors = F)
mturk<-read.csv("C:/Users/kevin/desktop/DLS.csv", stringsAsFactors = F)
library(tidyverse)
library(lme4)
mydata$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1
mturk$Race[mturk$Race |= "White" &  mturk$Race |= "Black or African American" & mturk$Race |= "Asian"]<-"Other"
mturk$Race[mturk$Race != "White" &  mturk$Race != "Black or African American" & mturk$Race != "Asian"]<-"Other"
mturk$Race
mturk$Race[mturk$Race == "White" &  mturk$Hispanic == "Yes"]<-"Hispanic"
mturk$Race
mturk$Overall..how.confident.do.you.feel.using.computers..smartphones..or.other.electronic.devices.to.do.the.things.you.need.to.do.online..
mturk[38,]
mturk$Race
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("Rcpp")
install.packages("Rcpp")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("digest")
install.packages("digest")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("rlang")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("testthat")
install.packages("testthat")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("backports")
install.packages("backports")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("processx")
install.packages("processx")
install.packages("ps")
install.packages("ps")
install.packages("ISOcodes")
install.packages("usethis")
install.packages("curl")
install.packages("fs")
install.packages("gh")
install.packages("git2r")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("quanteda")
devtools::install_github("kbenoit/sophistication", force=T)
library(quanteda)
devtools::install_github("kbenoit/sophistication", force=T)
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("ellipsis")
install.packages("ellipsis")
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("glue")
install.packages("glue")
install.packages("vctrs")
devtools::install_github("benjaminguinaudeau/tiktokr")
library(tiktokr)
library(reticulate)
use_python(py_config()$python)
Y
yes
use_python(py_config()$python)
install_tiktokr()
init_tiktokr()
trends <- get_trending(200)
user <- get_username("willsmith")
get_username("willsmith")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
readab <- textstat_readability(corpus_subset(data_corpus_inaugural, Year > 1980),
measure = "Flesch.Kincaid")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1900), stem = TRUE,
remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "relFreq")))
presCluster <- hclust(presDistMat)
presCluster$labels <- docnames(presDfm)
dev.new()
plot(presCluster)
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
txt
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
?textstat_simil
textstat_simil(presDfm, presDfm["1985-Reagan", ],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
summary(presDfm)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
presDfm
rm(list = ls())
library(dplyr)
library(caret)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_data <- readRDS("news_data.rds")
table(news_data$category)
news_samp <- news_data %>% filter(category %in% c("WEIRD NEWS", "GOOD NEWS")) %>% select(headline, category) %>% setNames(c("text", "class"))
dim(news_samp)
head(news_samp$text[news_samp$class == "WEIRD NEWS"])
head(news_samp$text[news_samp$class == "GOOD NEWS"])
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- dplyr::recode(news_samp$class,  "WEIRD NEWS" = "weird", "GOOD NEWS" = "good")
prop.table(table(news_samp$class))
set.seed(1984)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
nrow(news_samp)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
library(caret)
library(quanteda)
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
set.seed(1984)
1:nrow(news_dfm)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
baseline_acc <- max(prop.table(table(test_y)))
baseline_acc
trctrl <- trainControl(method = "none")
svm_mod_linear <- train(x = train_x,
y = train_y,
method = "svmLinear",
trControl = trctrl)
svm_linear_pred <- predict(svm_mod_linear, newdata = test_x)
svm_linear_cmat <- confusionMatrix(svm_linear_pred, test_y)
svm_linear_cmat
svm_mod_radial <- train(x = train_x,
y = train_y,
method = "svmRadial",
trControl = trctrl)
svm_radial_pred <- predict(svm_mod_radial, newdata = test_x)
svm_radial_cmat <- confusionMatrix(svm_radial_pred, test_y)
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"SVM-Linear Accuracy:",  svm_linear_cmat$overall[["Accuracy"]], "\n",
"SVM-Radial Accuracy:",  svm_radial_cmat$overall[["Accuracy"]]
)
rm(list = ls())
# load required libraries
library(dplyr)
library(randomForest)
library(mlbench)
library(caret)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_data <- readRDS("news_data.rds")
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
setwd("C:/Users/kevin/Documents/GtHub/TAD_2021/R lessons/")
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_samp <- news_data %>%
filter(category %in% c("MONEY", "LATINO VOICES")) %>%
group_by(category) %>%
sample_n(500) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
table(news_data$category)
head(news_samp$text[news_samp$class == "MONEY"])
head(news_samp$text[news_samp$class == "LATINO VOICES"])
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- recode(news_samp$class,  "MONEY" = "money", "LATINO VOICES" = "latino")
prop.table(table(news_samp$class))
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
library(caret)
library(quanteda)
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
presen_absent <- news_dfm
presen_absent <- news_dfm
presen_absent[presen_absent > 0] <- 1
feature_count <- apply(presen_absent, 2, sum)
features <- names(which(feature_count > 5))
features
news_dfm <- news_dfm[,features]
set.seed(1984)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
library(randomForest)
ncol(train_x)
mtry = sqrt(ncol(train_x))  # number of features to sample at each split
ntree = 51  # numbre of trees to grow
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
print(rf.base)
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry, nodesize = 10))
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry, nodesize = 10))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry, nodesize = 100))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
rm(list = ls())
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
install.packages("factoextra")
library(quanteda)
library(factoextra)
library(dplyr)
?prcomp # uses the singular value decomposition approach: examines the covariances/correlations between individuals
data("data_corpus_inaugural")
inaugural <- corpus_subset(data_corpus_inaugural, Year > "1900-01-01")
inaugural_dfm <- dfm(inaugural,
stem = T,
remove_punct = T,
remove = stopwords("english")
)
inaugural_mat <- convert(inaugural_dfm, to = "matrix") # convert to matrix
inaugural_pca <- prcomp(inaugural_mat, center = TRUE, scale = TRUE)
fviz_eig(inaugural_pca, addlabels = TRUE)
inaugural_pca$rotation[1:10, 1:5]
dim(inaugural_pca$rotation)
pc_loadings <- inaugural_pca$rotation
cor(pc_loadings[,1], pc_loadings[,2])  # these should be orthogonal
N <- 10
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
View(inaugural_pca$x)  # each observation
install.packages("text2vec")
library(text2vec)
nearest_neighbors <- function(query, low_dim_space, N = 5, norm = "l2"){
cos_sim <- sim2(x = low_dim_space, y = low_dim_space[query, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # query is always the nearest neighbor hence dropped
}
nearest_neighbors(query = "2009-Obama", low_dim_space = inaugural_pca$x, N = 5, norm = "l2")
rm(list = ls())
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
# Loading packages
#install.packages("factoextra")
library(quanteda)
library(factoextra)
library(dplyr)
## 1 PCA
# 1.1  function in base R for PCA:
# see: http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/
?prcomp # uses the singular value decomposition approach: examines the covariances/correlations between individuals
# Remember to center your data! (default = TRUE) -- use scale() on your matrix beforehand, or the option in prcomp()
# And don't have any missing values!
# 1.2 Example
data("data_corpus_inaugural")
inaugural <- corpus_subset(data_corpus_inaugural, Year > "1900-01-01")
inaugural_dfm <- dfm(inaugural,
stem = T,
remove_punct = T,
remove = stopwords("english")
)
inaugural_mat <- convert(inaugural_dfm, to = "matrix") # convert to matrix
# run pca
inaugural_pca <- prcomp(inaugural_mat, center = TRUE, scale = TRUE)
# visualize eigenvalues (scree plot: shows percentage of variance explained by each dimension)
fviz_eig(inaugural_pca, addlabels = TRUE)
# Loadings for each variable: columns contain the eigenvectors
inaugural_pca$rotation[1:10, 1:5]
dim(inaugural_pca$rotation)
# Q: can we interpret the dimensions?
# top loadings on PC1
pc_loadings <- inaugural_pca$rotation
# what do we expect this correlation to be?
cor(pc_loadings[,1], pc_loadings[,2])  # these should be orthogonal
# token loadings
N <- 10
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
# plot top tokens according to absolute loading values
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
# Value of the rotated data: your "new", dimensionality reduced data
View(inaugural_pca$x)  # each observation
# retrieve most similar documents
#install.packages("text2vec")
#library(text2vec)
# function computes cosine similarity between query and all documents and returns N most similar
nearest_neighbors <- function(query, low_dim_space, N = 5, norm = "l2"){
cos_sim <- sim2(x = low_dim_space, y = low_dim_space[query, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # query is always the nearest neighbor hence dropped
}
# apply to document retrieval
nearest_neighbors(query = "2009-Obama", low_dim_space = inaugural_pca$x, N = 5, norm = "l2")
##### Exercise ######
# Question 1: Update the code so that it calculates the terms with the
#top loadings on PC2. What is the theme of this dimension?
# Question 2: Who are the 5 people Obama's inaugural address is most
#close to in 2013? What about Trump in 2017?
