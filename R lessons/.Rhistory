sotu_mean <- mean(stat)
sotu_var <- var(stat)
sotu_cs_test <- cs.test(stat)
sotu_ts <- as.ts(stat)
sotu_breaks <- breakpoints(sotu_ts ~ 1 )
summary(sotu_breaks)
#3 breakpoints best fit the data
fm0 <- lm(stat ~ breakfactor(sotu_breaks, breaks = 3))
sotu_breaks <- breakpoints(sotu_ts ~ 1, breaks = 3 )
year_breaks <- year[breakdates(sotu_breaks)]
## plot the trend
require(ggplot2)
p <- ggplot(data = docvars(data_corpus_SOTU),
aes(x = year, y = stat)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
xlab("") +
ylab("Flesch Reading Ease Score") +
geom_errorbar(aes(ymin=stat-1.96*bs_sd, ymax=stat+1.96*bs_sd), colour="grey70", width=.1) +
geom_point(aes(shape = party), size = 2, fill = "black") +
scale_shape_manual(values = c(17, 8, 3, 19, 15, 20)) +
geom_vline(aes(xintercept= year_breaks[1] ) ) +
geom_vline(aes(xintercept= year_breaks[2] ) ) +
geom_vline(aes(xintercept= year_breaks[3] ) ) +
theme(plot.title = element_text(lineheight=.8, face="bold"),
axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))
print(p)
## Figure 2
# compute the levels
period <- cut(year, breaks = c(min(year), year_breaks, max(year)), include.lowest = TRUE, dig.lab = 4)
# reformat the levels
levs <- stringi::stri_replace_all_fixed(levels(period), c("[", "]", "("), "", vectorize_all = FALSE)
levs <- strsplit(levs, ",")
levs[2:length(levs)] <- lapply(levs[2:length(levs)], function(x) {
x[1] <- as.integer(x[1]) + 1
x
})
levels(period) <- sapply(levs, paste, collapse = "-")
ggplot(aes(y = stat, x = period, fill = docvars(data_corpus_SOTU, "delivery")),
data = NULL) +
geom_boxplot() +
scale_fill_grey("", start = .6, end = .9) +
theme(plot.title = element_text(lineheight=.8, face="bold"),
axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15)) +
labs(y = "Flesch Reading Ease Score", x = "") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
legend.key.size =  unit(2.5, "cm"),
axis.line = element_line(colour = "black"))
##Figures 3, 5 and 7: US data
library(sophistication)
library(quanteda)
library(stringi)
library(magrittr)
library(dplyr)
data("data_corpus_SOTU")
SOTU_stat <- textstat_readability(data_corpus_SOTU, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
SOTU_year <- lubridate::year(docvars(data_corpus_SOTU, "Date"))
SOTU_fre_df <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$Flesch)
SOTU_fre_sent <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanSentenceLength)
SOTU_fre_word <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanWordSyllables)
###EO
data("data_corpus_eo")
#drop the ones that are empty
clean1 <- data_corpus_eo$documents
##problems
clean1$texts<-stri_replace_all_fixed(clean1$texts, "A. D.", "AD")
#drop anything that's very long (95th percentile and above)
quant95 <- which( nchar(clean1$texts) >
quantile( nchar(clean1$texts), prob=.95) )
clean2 <- clean1[-quant95, c(1:2)]
eo_corp <- corpus(clean2, text_field = "texts")
##drop very short sentences
eo_corp <- corpus_trimsentences(eo_corp, min_length = 4)
eo_corp <- corpus_subset(eo_corp, ntoken(eo_corp) > 10)
eo_year <- docvars(eo_corp)
eo_stat <- textstat_readability(eo_corp$documents$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
eo_fre_df <- data.frame("year" = eo_year$Year, "eo_stat" = eo_stat$Flesch)
eo_fre_sent <- data.frame("year" = eo_year$Year, "eo_stat" = eo_stat$meanSentenceLength)
eo_fre_word <- data.frame("year" = eo_year$Year, "eo_stat" = eo_stat$meanWordSyllables)
########SCOTUS
#load("C:/Users/kevin/Documents/GitHub/BMS_chapter_replication/data/data_corpus_SCOTUS.rda")
load("C:/Users/kevin/Desktop/data_corpus_SCOTUS.rda")
#drop the ones that are empty
clean1 <- corpus_subset(data_corpus_SCOTUS, nchar(texts(data_corpus_SCOTUS)) > 0)
## implement fixes
texts(clean1) <- stri_replace_all_fixed(texts(clean1), "U. S.", "US")
#drop anything that's very long (95th percentile and above)
temp_lengths <- stri_length(texts(clean1))
clean2 <- corpus_subset(clean1, temp_lengths <quantile(temp_lengths, prob = .95))
scotus_lengths<-ntoken(clean2, removePunct = T)
clean3 <- corpus_trimsentences(clean2, min_length = 4)
scotus_year <- clean3$documents$Year
scotus_stat <- textstat_readability(clean3$documents$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
scotus_fre_df <- data.frame("year" = scotus_year, "scotus_stat" = scotus_stat$Flesch)
scotus_fre_sent <- data.frame("year" = scotus_year, "scotus_stat" = scotus_stat$meanSentenceLength)
scotus_fre_word <- data.frame("year" = scotus_year, "scotus_stat" = scotus_stat$meanWordSyllables)
##too many to plot well, and very unbalanced (way more modern ones)--try for a more balanced sample
years<-unique(scotus_fre_df$year)
indices<-vector()
for(i in 1:length(years)){
set<-which(scotus_fre_df[,"year"]==years[i])
num<-min(length(set), 30)
samp<-sample(set, num, replace = FALSE)
indices<-c(indices, samp)
}
balanced_scotus_fre_df<-scotus_fre_df[indices,]
balanced_scotus_fre_sent<-scotus_fre_sent[indices,]
balanced_scotus_fre_word<-scotus_fre_word[indices,]
require(reshape2)
###Congress
load("C:/Users/kevin/Desktop/data_corpus_CR.rdata")
#drop the ones that are by the speaker
speaker <- which( (data_corpus_CR$documents$name)=="Speaker" )
clean1 <- data_corpus_CR$documents[-speaker, ]
#drop anything that's very long (90th percentile and above)
quant95 <- which( nchar(clean1$texts) >
quantile( nchar(clean1$texts), prob=.95) )
clean2 <- clean1[-quant95, ]
##sync up year format
clean2$year[clean2$year==95]<-1995
clean2$year[clean2$year==96]<-1996
clean2$year[clean2$year==97]<-1997
clean2$year[clean2$year==98]<-1998
clean2$year[clean2$year==99]<-1999
clean2$year[clean2$year==0]<-2000
clean2$year[clean2$year==1]<-2001
clean2$year[clean2$year==2]<-2002
clean2$year[clean2$year==3]<-2003
clean2$year[clean2$year==4]<-2004
clean2$year[clean2$year==5]<-2005
clean2$year[clean2$year==6]<-2006
clean2$year[clean2$year==7]<-2007
clean2$year[clean2$year==8]<-2008
##take a very small sample--can't run readability on the whole thing
sample<-sample(seq(1,length(clean2$texts)), 10000, replace = F  )
sample_data<-clean2[sample,]
congress_year <- sample_data$year
congress_stat <- textstat_readability(sample_data$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
congress_fre_df <- data.frame("year" = congress_year, "congress_stat" = congress_stat$Flesch)
congress_fre_sent <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanSentenceLength)
congress_fre_word <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanWordSyllables)
##melt together for plotting--Figure 3
require(reshape2)
df_US<-melt(list(SOTU=SOTU_fre_df, SCOTUS=balanced_scotus_fre_df, Congress = congress_fre_df,
ExecOrders = eo_fre_df), id.vars="year")
require(ggplot2)
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(-20, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Flesch Reading Ease Score") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1995, y = 45, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1800, y = 20, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 50, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1970, y = 0, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 5
df_US_sent<-melt(list(SOTU=SOTU_fre_sent, SCOTUS=balanced_scotus_fre_sent, Congress = congress_fre_sent,
ExecOrders = eo_fre_sent), id.vars="year")
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US_sent,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(15, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Sentence Length") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1960, y = 17, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1910, y = 25, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 30, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1960, y = 50, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 7
df_US_word<-melt(list(SOTU=SOTU_fre_word, SCOTUS=balanced_scotus_fre_word, Congress = congress_fre_word,
ExecOrders = eo_fre_word), id.vars="year")
p <- ggplot(data = df_US_word,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(1.4, 1.85)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Number of Syllables per Word") + theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "SOTU", x = 1800, y = 1.73, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 1.47, size = 6, colour = "black") +
annotate("text", label = "Con. Speech", x = 1998, y = 1.61, size = 6, colour = "black")+
annotate("text", label = "Executive Orders", x = 1950, y = 1.82, size = 6, colour = "black")
print(p)
##Figures 4, 6 and 8: Comparative data
data("data_corpus_SOTU")
SOTU_stat <- textstat_readability(data_corpus_SOTU, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
SOTU_year <- lubridate::year(docvars(data_corpus_SOTU, "Date"))
SOTU_fre_df <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$Flesch)
SOTU_fre_sent <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanSentenceLength)
SOTU_fre_word <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanWordSyllables)
########nobel
#load("data_text/NobelLitePresentations/data_corpus_nobel.rdata")
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
docvars(data_corpus_nobel, "Date")
docvars(data_corpus_nobel)
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "year"))
nobel_year <- (docvars(data_corpus_nobel, "year"))
install.packages("randomizr", repos="http://r.declaredesign.org")
library("randomizr", lib.loc="~/R/win-library/3.4")
remove.packages("randomizr", lib="~/R/win-library/3.4")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr", repos = "http://r.declaredesign.org")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr")
library("randomizr", lib.loc="~/R/win-library/3.4")
library(DeclareDesign)
library(quanteda)
library(sophistication)
data("data_corpus_Crimson")
library(dplyr)
require(stringr)
require(data.table)
##read in
load("C:/Users/kevin/Documents/GitHub/sophistication-papers/analysis_article/output/fitted_BT_model.Rdata")
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
data("data_corpus_SOTU")
library(quanteda)
install.packages("quanteda")
install.packages("quanteda")
library(quanteda)
data_corpus_SOTU
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
library(sophistication)
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
library(quanteda.quanteda)
library(quanteda.corpora)
install.packages("quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora", force = TRUE)
install.packages("digest")
library(readtext)
xx<-readtext(xx.txt)
xx<-readtext("xx.txt")
xx<-readtext(file ="xx.txt")
?readtext
xx<-readtext(file ="xx")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv", stringsAsFactors = F)
mturk<-read.csv("C:/Users/kevin/desktop/DLS.csv", stringsAsFactors = F)
library(tidyverse)
library(lme4)
mydata$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1
mturk$Race[mturk$Race |= "White" &  mturk$Race |= "Black or African American" & mturk$Race |= "Asian"]<-"Other"
mturk$Race[mturk$Race != "White" &  mturk$Race != "Black or African American" & mturk$Race != "Asian"]<-"Other"
mturk$Race
mturk$Race[mturk$Race == "White" &  mturk$Hispanic == "Yes"]<-"Hispanic"
mturk$Race
mturk$Overall..how.confident.do.you.feel.using.computers..smartphones..or.other.electronic.devices.to.do.the.things.you.need.to.do.online..
mturk[38,]
mturk$Race
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("Rcpp")
install.packages("Rcpp")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("digest")
install.packages("digest")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("rlang")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("testthat")
install.packages("testthat")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("backports")
install.packages("backports")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("processx")
install.packages("processx")
install.packages("ps")
install.packages("ps")
install.packages("ISOcodes")
install.packages("usethis")
install.packages("curl")
install.packages("fs")
install.packages("gh")
install.packages("git2r")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("quanteda")
devtools::install_github("kbenoit/sophistication", force=T)
library(quanteda)
devtools::install_github("kbenoit/sophistication", force=T)
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("ellipsis")
install.packages("ellipsis")
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("glue")
install.packages("glue")
install.packages("vctrs")
devtools::install_github("benjaminguinaudeau/tiktokr")
library(tiktokr)
library(reticulate)
use_python(py_config()$python)
Y
yes
use_python(py_config()$python)
install_tiktokr()
init_tiktokr()
trends <- get_trending(200)
user <- get_username("willsmith")
get_username("willsmith")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
readab <- textstat_readability(corpus_subset(data_corpus_inaugural, Year > 1980),
measure = "Flesch.Kincaid")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1900), stem = TRUE,
remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "relFreq")))
presCluster <- hclust(presDistMat)
presCluster$labels <- docnames(presDfm)
dev.new()
plot(presCluster)
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
txt
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
?textstat_simil
textstat_simil(presDfm, presDfm["1985-Reagan", ],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
summary(presDfm)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
presDfm
rm(list = ls())
library(dplyr)
library(caret)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_data <- readRDS("news_data.rds")
table(news_data$category)
news_samp <- news_data %>% filter(category %in% c("WEIRD NEWS", "GOOD NEWS")) %>% select(headline, category) %>% setNames(c("text", "class"))
dim(news_samp)
head(news_samp$text[news_samp$class == "WEIRD NEWS"])
head(news_samp$text[news_samp$class == "GOOD NEWS"])
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- dplyr::recode(news_samp$class,  "WEIRD NEWS" = "weird", "GOOD NEWS" = "good")
prop.table(table(news_samp$class))
set.seed(1984)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
nrow(news_samp)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
library(caret)
library(quanteda)
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
set.seed(1984)
1:nrow(news_dfm)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
baseline_acc <- max(prop.table(table(test_y)))
baseline_acc
trctrl <- trainControl(method = "none")
svm_mod_linear <- train(x = train_x,
y = train_y,
method = "svmLinear",
trControl = trctrl)
svm_linear_pred <- predict(svm_mod_linear, newdata = test_x)
svm_linear_cmat <- confusionMatrix(svm_linear_pred, test_y)
svm_linear_cmat
svm_mod_radial <- train(x = train_x,
y = train_y,
method = "svmRadial",
trControl = trctrl)
svm_radial_pred <- predict(svm_mod_radial, newdata = test_x)
svm_radial_cmat <- confusionMatrix(svm_radial_pred, test_y)
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"SVM-Linear Accuracy:",  svm_linear_cmat$overall[["Accuracy"]], "\n",
"SVM-Radial Accuracy:",  svm_radial_cmat$overall[["Accuracy"]]
)
rm(list = ls())
# load required libraries
library(dplyr)
library(randomForest)
library(mlbench)
library(caret)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_data <- readRDS("news_data.rds")
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
setwd("C:/Users/kevin/Documents/GtHub/TAD_2021/R lessons/")
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_samp <- news_data %>%
filter(category %in% c("MONEY", "LATINO VOICES")) %>%
group_by(category) %>%
sample_n(500) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
table(news_data$category)
head(news_samp$text[news_samp$class == "MONEY"])
head(news_samp$text[news_samp$class == "LATINO VOICES"])
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- recode(news_samp$class,  "MONEY" = "money", "LATINO VOICES" = "latino")
prop.table(table(news_samp$class))
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
library(caret)
library(quanteda)
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
presen_absent <- news_dfm
presen_absent <- news_dfm
presen_absent[presen_absent > 0] <- 1
feature_count <- apply(presen_absent, 2, sum)
features <- names(which(feature_count > 5))
features
news_dfm <- news_dfm[,features]
set.seed(1984)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
library(randomForest)
ncol(train_x)
mtry = sqrt(ncol(train_x))  # number of features to sample at each split
ntree = 51  # numbre of trees to grow
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
print(rf.base)
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry, nodesize = 10))
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry, nodesize = 10))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry, nodesize = 100))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
