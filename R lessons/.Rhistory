clean2$year[clean2$year==0]<-2000
clean2$year[clean2$year==1]<-2001
clean2$year[clean2$year==2]<-2002
clean2$year[clean2$year==3]<-2003
clean2$year[clean2$year==4]<-2004
clean2$year[clean2$year==5]<-2005
clean2$year[clean2$year==6]<-2006
clean2$year[clean2$year==7]<-2007
clean2$year[clean2$year==8]<-2008
##take a very small sample--can't run readability on the whole thing
sample<-sample(seq(1,length(clean2$texts)), 10000, replace = F  )
sample_data<-clean2[sample,]
congress_year <- sample_data$year
congress_stat <- textstat_readability(sample_data$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
congress_fre_df <- data.frame("year" = congress_year, "congress_stat" = congress_stat$Flesch)
congress_fre_sent <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanSentenceLength)
congress_fre_word <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanWordSyllables)
##melt together for plotting--Figure 3
require(reshape2)
df_US<-melt(list(SOTU=SOTU_fre_df, SCOTUS=balanced_scotus_fre_df, Congress = congress_fre_df,
ExecOrders = eo_fre_df), id.vars="year")
require(ggplot2)
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(-20, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Flesch Reading Ease Score") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1995, y = 45, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1800, y = 20, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 50, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1970, y = 0, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 5
df_US_sent<-melt(list(SOTU=SOTU_fre_sent, SCOTUS=balanced_scotus_fre_sent, Congress = congress_fre_sent,
ExecOrders = eo_fre_sent), id.vars="year")
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US_sent,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(15, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Sentence Length") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1960, y = 17, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1910, y = 25, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 30, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1960, y = 50, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 7
df_US_word<-melt(list(SOTU=SOTU_fre_word, SCOTUS=balanced_scotus_fre_word, Congress = congress_fre_word,
ExecOrders = eo_fre_word), id.vars="year")
p <- ggplot(data = df_US_word,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(1.4, 1.85)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Number of Syllables per Word") + theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "SOTU", x = 1800, y = 1.73, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 1.47, size = 6, colour = "black") +
annotate("text", label = "Con. Speech", x = 1998, y = 1.61, size = 6, colour = "black")+
annotate("text", label = "Executive Orders", x = 1950, y = 1.82, size = 6, colour = "black")
print(p)
##Figures 4, 6 and 8: Comparative data
data("data_corpus_SOTU")
SOTU_stat <- textstat_readability(data_corpus_SOTU, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
SOTU_year <- lubridate::year(docvars(data_corpus_SOTU, "Date"))
SOTU_fre_df <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$Flesch)
SOTU_fre_sent <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanSentenceLength)
SOTU_fre_word <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanWordSyllables)
########nobel
#load("data_text/NobelLitePresentations/data_corpus_nobel.rdata")
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
docvars(data_corpus_nobel, "Date")
docvars(data_corpus_nobel)
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "year"))
nobel_year <- (docvars(data_corpus_nobel, "year"))
install.packages("randomizr", repos="http://r.declaredesign.org")
library("randomizr", lib.loc="~/R/win-library/3.4")
remove.packages("randomizr", lib="~/R/win-library/3.4")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr", repos = "http://r.declaredesign.org")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr")
library("randomizr", lib.loc="~/R/win-library/3.4")
library(DeclareDesign)
library(quanteda)
library(sophistication)
data("data_corpus_Crimson")
library(dplyr)
require(stringr)
require(data.table)
##read in
load("C:/Users/kevin/Documents/GitHub/sophistication-papers/analysis_article/output/fitted_BT_model.Rdata")
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
data("data_corpus_SOTU")
library(quanteda)
install.packages("quanteda")
install.packages("quanteda")
library(quanteda)
data_corpus_SOTU
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
library(sophistication)
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
library(quanteda.quanteda)
library(quanteda.corpora)
install.packages("quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora", force = TRUE)
install.packages("digest")
library(readtext)
xx<-readtext(xx.txt)
xx<-readtext("xx.txt")
xx<-readtext(file ="xx.txt")
?readtext
xx<-readtext(file ="xx")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv")
mturk<-read.csv("C:/Users/kevin/desktop/Digital Literacy Survey.csv", stringsAsFactors = F)
mturk<-read.csv("C:/Users/kevin/desktop/DLS.csv", stringsAsFactors = F)
library(tidyverse)
library(lme4)
mydata$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1<-cut(mturk$Age, c(18,29,49,64,99))
mturk$Agecat1
mturk$Race[mturk$Race |= "White" &  mturk$Race |= "Black or African American" & mturk$Race |= "Asian"]<-"Other"
mturk$Race[mturk$Race != "White" &  mturk$Race != "Black or African American" & mturk$Race != "Asian"]<-"Other"
mturk$Race
mturk$Race[mturk$Race == "White" &  mturk$Hispanic == "Yes"]<-"Hispanic"
mturk$Race
mturk$Overall..how.confident.do.you.feel.using.computers..smartphones..or.other.electronic.devices.to.do.the.things.you.need.to.do.online..
mturk[38,]
mturk$Race
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("Rcpp")
install.packages("Rcpp")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("digest")
install.packages("digest")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("glue")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("rlang")
install.packages("rlang")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("testthat")
install.packages("testthat")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("backports")
install.packages("backports")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("processx")
install.packages("processx")
install.packages("ps")
install.packages("ps")
install.packages("ISOcodes")
install.packages("usethis")
install.packages("curl")
install.packages("fs")
install.packages("gh")
install.packages("git2r")
devtools::install_github("kbenoit/sophistication", force=T)
install.packages("quanteda")
devtools::install_github("kbenoit/sophistication", force=T)
library(quanteda)
devtools::install_github("kbenoit/sophistication", force=T)
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("ellipsis")
install.packages("ellipsis")
devtools::install_github("benjaminguinaudeau/tiktokr")
install.packages("glue")
install.packages("glue")
install.packages("vctrs")
devtools::install_github("benjaminguinaudeau/tiktokr")
library(tiktokr)
library(reticulate)
use_python(py_config()$python)
Y
yes
use_python(py_config()$python)
install_tiktokr()
init_tiktokr()
trends <- get_trending(200)
user <- get_username("willsmith")
get_username("willsmith")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
readab <- textstat_readability(corpus_subset(data_corpus_inaugural, Year > 1980),
measure = "Flesch.Kincaid")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1900), stem = TRUE,
remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "relFreq")))
presCluster <- hclust(presDistMat)
presCluster$labels <- docnames(presDfm)
dev.new()
plot(presCluster)
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
txt
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
?textstat_simil
textstat_simil(presDfm, presDfm["1985-Reagan", ],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
summary(presDfm)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
presDfm
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
rm(list = ls())
# load required libraries
library(dplyr)
library(caret)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_data <- readRDS("news_data.rds")
table(news_data$category)
news_samp <- news_data %>% filter(category %in% c("WEIRD NEWS", "GOOD NEWS")) %>% select(headline, category) %>% setNames(c("text", "class"))
dim(news_samp)
head(news_samp$text[news_samp$class == "WEIRD NEWS"])
head(news_samp$text[news_samp$class == "GOOD NEWS"])
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- dplyr::recode(news_samp$class,  "WEIRD NEWS" = "weird", "GOOD NEWS" = "good")
# what's the distribution of classes?
prop.table(table(news_samp$class))
# randomize order (notice how we split below)
set.seed(1984)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
#----------------------------------------
# 4. Support Vector Machine (SVM) using Caret ---
#----------------------------------------
library(caret)
library(quanteda)
# create document feature matrix
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
# A. the caret package has it's own partitioning function
set.seed(1984)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
# baseline
baseline_acc <- max(prop.table(table(test_y)))
# B. define training options (we've done this manually above)
trctrl <- trainControl(method = "none")
#trctrl <- trainControl(method = "LOOCV", p = 0.8)
?trainControl
# C. train model (caret gives us access to even more options)
# see: https://topepo.github.io/caret/available-models.html
# svm - linear
svm_mod_linear <- train(x = train_x,
y = train_y,
method = "svmLinear",
trControl = trctrl)
svm_linear_pred <- predict(svm_mod_linear, newdata = test_x)
svm_linear_cmat <- confusionMatrix(svm_linear_pred, test_y)
# svm - radial
svm_mod_radial <- train(x = train_x,
y = train_y,
method = "svmRadial",
trControl = trctrl)
svm_radial_pred <- predict(svm_mod_radial, newdata = test_x)
svm_radial_cmat <- confusionMatrix(svm_radial_pred, test_y)
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"SVM-Linear Accuracy:",  svm_linear_cmat$overall[["Accuracy"]], "\n",
"SVM-Radial Accuracy:",  svm_radial_cmat$overall[["Accuracy"]]
)
####Be sure to save it as a new file, with a new filename!
# 1. Re-run the analysis with a smaller training set and larger test set. Does the accuracy go up or down?
# clear global environment
rm(list = ls())
# load required libraries
library(dplyr)
library(randomForest)
library(mlbench)
library(caret)
install.packages("mlbench")
library(mlbench)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_data <- readRDS("news_data.rds")
table(news_data$category)
# let's work with 2 categories
set.seed(1984)
news_samp <- news_data %>%
filter(category %in% c("MONEY", "LATINO VOICES")) %>%
group_by(category) %>%
sample_n(500) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
# get a sense of how the text looks
dim(news_samp)
head(news_samp$text[news_samp$class == "MONEY"])
head(news_samp$text[news_samp$class == "LATINO VOICES"])
# some pre-processing (the rest we'll let dfm do)
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- recode(news_samp$class,  "MONEY" = "money", "LATINO VOICES" = "latino")
# what's the distribution of classes?
prop.table(table(news_samp$class))
# randomize order (notice how we split below)
set.seed(1984)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
#----------------------------------------
# 2. Prepare Data                        ---
#----------------------------------------
library(caret)
library(quanteda)
# create document feature matrix
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
# keep tokens that appear in at least 5 headlines
presen_absent <- news_dfm
presen_absent[presen_absent > 0] <- 1
feature_count <- apply(presen_absent, 2, sum)
features <- names(which(feature_count > 5))
news_dfm <- news_dfm[,features]
# caret package has it's own partitioning function
set.seed(1984)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
#----------------------------------------
# 3. Using RandomForest                  ---
#----------------------------------------
library(randomForest)
mtry = sqrt(ncol(train_x))  # number of features to sample at each split
ntree = 51  # numbre of trees to grow
# more trees generally improve accuracy but at the cost of computation time
# odd numbers avoid ties (recall default aggregation is "majority voting")
set.seed(1984)
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
print(rf.base)
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
rm(list = ls())
# load required libraries
library(dplyr)
library(randomForest)
library(mlbench)
library(caret)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
#----------------------------------------
# 1. Load, clean and inspect data        ---
#----------------------------------------
news_data <- readRDS("news_data.rds")
table(news_data$category)
# let's work with 2 categories
set.seed(1984)
news_samp <- news_data %>%
filter(category %in% c("MONEY", "LATINO VOICES")) %>%
group_by(category) %>%
sample_n(500) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
# get a sense of how the text looks
dim(news_samp)
head(news_samp$text[news_samp$class == "MONEY"])
head(news_samp$text[news_samp$class == "LATINO VOICES"])
# some pre-processing (the rest we'll let dfm do)
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- dplyr::recode(news_samp$class,  "MONEY" = "money", "LATINO VOICES" = "latino")
# what's the distribution of classes?
prop.table(table(news_samp$class))
# randomize order (notice how we split below)
set.seed(1984)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
#----------------------------------------
# 2. Prepare Data                        ---
#----------------------------------------
library(caret)
library(quanteda)
# create document feature matrix
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
# keep tokens that appear in at least 5 headlines
presen_absent <- news_dfm
presen_absent[presen_absent > 0] <- 1
feature_count <- apply(presen_absent, 2, sum)
features <- names(which(feature_count > 5))
news_dfm <- news_dfm[,features]
# caret package has it's own partitioning function
set.seed(1984)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
#----------------------------------------
# 4. RandomForest Using Caret            ---
#----------------------------------------
trainControl <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"
mtry <- sqrt(ncol(train_x))
ntree = 51
tunegrid <- expand.grid(.mtry = mtry)
set.seed(1984)
system.time(rf.caret <- train(x = train_x, y = train_y, method = "rf", metric = metric, tuneGrid = tunegrid, trControl = trainControl,
ntree = ntree))
# print results
print(rf.caret)
# plot importance
varImpPlot(rf.caret$finalModel, n.var = 10, main = "Variable Importance")
#----------------------------------------
# 5. RandomForest Using Caret + tuning   ---
#----------------------------------------
# note: the package RandomForest also has its own tuning function: tuneRF
trainControl <- trainControl(method = "cv", number = 5)
metric <- "Accuracy"
tunegrid <- expand.grid(.mtry = c(0.5*mtry, mtry, 1.5*mtry))  # at the moment caret only allows tuning of mtry (partly b/c ntree is just a matter of computational constratints)
set.seed(1984)
system.time(rf.grid <- train(x = train_x, y = train_y, method = "rf", metric = metric, tuneGrid = tunegrid, trControl = trainControl,
ntree = ntree))
# print grid search results
print(rf.grid)
# plot grid search results
plot(rf.grid)
#----------------------------------------
# 6. RandomForest Using Caret + manual tuning ---
#----------------------------------------
mtry <- sqrt(ncol(train_x))
tunegrid <- expand.grid(.mtry = mtry)
# ntree = 1
set.seed(1984)
system.time(rf.man1 <- train(x = train_x, y = train_y, method = "rf", metric = metric, tuneGrid = tunegrid, trControl = trainControl, ntree = 1))
# ntree = 5
set.seed(1984)
system.time(rf.man2 <- train(x = train_x, y = train_y, method = "rf", metric = metric, tuneGrid = tunegrid, trControl = trainControl, ntree = 5))
# ntree = 51
set.seed(1984)
system.time(rf.man3 <- train(x = train_x, y = train_y, method = "rf", metric = metric, tuneGrid = tunegrid, trControl = trainControl, ntree = 51))
# collect results & summarize
results <- resamples(list(rf1 = rf.man1, rf5 = rf.man2, rf51 = rf.man3))
summary(results)
# test set accuracy
confusionMatrix(predict(rf.man1, newdata = test_x), test_y)
confusionMatrix(predict(rf.man2, newdata = test_x), test_y)
confusionMatrix(predict(rf.man3, newdata = test_x), test_y)
# box and whisker plots to compare models
scales <- list(x = list(relation = "free"), y = list(relation = "free"))
dev.new()
bwplot(results, scales = scales)
