nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
readab <- textstat_readability(corpus_subset(data_corpus_inaugural, Year > 1980),
measure = "Flesch.Kincaid")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1900), stem = TRUE,
remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "relFreq")))
presCluster <- hclust(presDistMat)
presCluster$labels <- docnames(presDfm)
dev.new()
plot(presCluster)
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
txt
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
?textstat_simil
textstat_simil(presDfm, presDfm["1985-Reagan", ],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
summary(presDfm)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
presDfm
rm(list = ls())
library(dplyr)
library(caret)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_data <- readRDS("news_data.rds")
table(news_data$category)
news_samp <- news_data %>% filter(category %in% c("WEIRD NEWS", "GOOD NEWS")) %>% select(headline, category) %>% setNames(c("text", "class"))
dim(news_samp)
head(news_samp$text[news_samp$class == "WEIRD NEWS"])
head(news_samp$text[news_samp$class == "GOOD NEWS"])
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- dplyr::recode(news_samp$class,  "WEIRD NEWS" = "weird", "GOOD NEWS" = "good")
prop.table(table(news_samp$class))
set.seed(1984)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
nrow(news_samp)
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
library(caret)
library(quanteda)
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
set.seed(1984)
1:nrow(news_dfm)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
baseline_acc <- max(prop.table(table(test_y)))
baseline_acc
trctrl <- trainControl(method = "none")
svm_mod_linear <- train(x = train_x,
y = train_y,
method = "svmLinear",
trControl = trctrl)
svm_linear_pred <- predict(svm_mod_linear, newdata = test_x)
svm_linear_cmat <- confusionMatrix(svm_linear_pred, test_y)
svm_linear_cmat
svm_mod_radial <- train(x = train_x,
y = train_y,
method = "svmRadial",
trControl = trctrl)
svm_radial_pred <- predict(svm_mod_radial, newdata = test_x)
svm_radial_cmat <- confusionMatrix(svm_radial_pred, test_y)
cat(
"Baseline Accuracy: ", baseline_acc, "\n",
"SVM-Linear Accuracy:",  svm_linear_cmat$overall[["Accuracy"]], "\n",
"SVM-Radial Accuracy:",  svm_radial_cmat$overall[["Accuracy"]]
)
rm(list = ls())
# load required libraries
library(dplyr)
library(randomForest)
library(mlbench)
library(caret)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_data <- readRDS("news_data.rds")
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
setwd("C:/Users/kevin/Documents/GtHub/TAD_2021/R lessons/")
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
news_samp <- news_data %>%
filter(category %in% c("MONEY", "LATINO VOICES")) %>%
group_by(category) %>%
sample_n(500) %>%  # sample 250 of each to reduce computation time (for lab purposes)
ungroup() %>%
select(headline, category) %>%
setNames(c("text", "class"))
table(news_data$category)
head(news_samp$text[news_samp$class == "MONEY"])
head(news_samp$text[news_samp$class == "LATINO VOICES"])
news_samp$text <- gsub(pattern = "'", "", news_samp$text)  # replace apostrophes
news_samp$class <- recode(news_samp$class,  "MONEY" = "money", "LATINO VOICES" = "latino")
prop.table(table(news_samp$class))
news_samp <- news_samp %>% sample_n(nrow(news_samp))
rownames(news_samp) <- NULL
library(caret)
library(quanteda)
news_dfm <- dfm(news_samp$text, stem = TRUE, remove_punct = TRUE, remove = stopwords("english")) %>% convert("matrix")
presen_absent <- news_dfm
presen_absent <- news_dfm
presen_absent[presen_absent > 0] <- 1
feature_count <- apply(presen_absent, 2, sum)
features <- names(which(feature_count > 5))
features
news_dfm <- news_dfm[,features]
set.seed(1984)
ids_train <- createDataPartition(1:nrow(news_dfm), p = 0.8, list = FALSE, times = 1)
train_x <- news_dfm[ids_train, ] %>% as.data.frame() # train set data
train_y <- news_samp$class[ids_train] %>% as.factor()  # train set labels
test_x <- news_dfm[-ids_train, ]  %>% as.data.frame() # test set data
test_y <- news_samp$class[-ids_train] %>% as.factor() # test set labels
library(randomForest)
ncol(train_x)
mtry = sqrt(ncol(train_x))  # number of features to sample at each split
ntree = 51  # numbre of trees to grow
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
print(rf.base)
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry, nodesize = 10))
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry, nodesize = 10))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
system.time(rf.base <- randomForest(x = train_x, y = train_y, ntree = ntree, mtry = mtry, nodesize = 100))
token_importance <- round(importance(rf.base, 2), 2)
head(rownames(token_importance)[order(-token_importance)])
varImpPlot(rf.base, n.var = 10, main = "Variable Importance")
rm(list = ls())
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
install.packages("factoextra")
library(quanteda)
library(factoextra)
library(dplyr)
?prcomp # uses the singular value decomposition approach: examines the covariances/correlations between individuals
data("data_corpus_inaugural")
inaugural <- corpus_subset(data_corpus_inaugural, Year > "1900-01-01")
inaugural_dfm <- dfm(inaugural,
stem = T,
remove_punct = T,
remove = stopwords("english")
)
inaugural_mat <- convert(inaugural_dfm, to = "matrix") # convert to matrix
inaugural_pca <- prcomp(inaugural_mat, center = TRUE, scale = TRUE)
fviz_eig(inaugural_pca, addlabels = TRUE)
inaugural_pca$rotation[1:10, 1:5]
dim(inaugural_pca$rotation)
pc_loadings <- inaugural_pca$rotation
cor(pc_loadings[,1], pc_loadings[,2])  # these should be orthogonal
N <- 10
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
View(inaugural_pca$x)  # each observation
install.packages("text2vec")
library(text2vec)
nearest_neighbors <- function(query, low_dim_space, N = 5, norm = "l2"){
cos_sim <- sim2(x = low_dim_space, y = low_dim_space[query, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # query is always the nearest neighbor hence dropped
}
nearest_neighbors(query = "2009-Obama", low_dim_space = inaugural_pca$x, N = 5, norm = "l2")
rm(list = ls())
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
# Loading packages
#install.packages("factoextra")
library(quanteda)
library(factoextra)
library(dplyr)
## 1 PCA
# 1.1  function in base R for PCA:
# see: http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/
?prcomp # uses the singular value decomposition approach: examines the covariances/correlations between individuals
# Remember to center your data! (default = TRUE) -- use scale() on your matrix beforehand, or the option in prcomp()
# And don't have any missing values!
# 1.2 Example
data("data_corpus_inaugural")
inaugural <- corpus_subset(data_corpus_inaugural, Year > "1900-01-01")
inaugural_dfm <- dfm(inaugural,
stem = T,
remove_punct = T,
remove = stopwords("english")
)
inaugural_mat <- convert(inaugural_dfm, to = "matrix") # convert to matrix
# run pca
inaugural_pca <- prcomp(inaugural_mat, center = TRUE, scale = TRUE)
# visualize eigenvalues (scree plot: shows percentage of variance explained by each dimension)
fviz_eig(inaugural_pca, addlabels = TRUE)
# Loadings for each variable: columns contain the eigenvectors
inaugural_pca$rotation[1:10, 1:5]
dim(inaugural_pca$rotation)
# Q: can we interpret the dimensions?
# top loadings on PC1
pc_loadings <- inaugural_pca$rotation
# what do we expect this correlation to be?
cor(pc_loadings[,1], pc_loadings[,2])  # these should be orthogonal
# token loadings
N <- 10
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
# plot top tokens according to absolute loading values
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
# Value of the rotated data: your "new", dimensionality reduced data
View(inaugural_pca$x)  # each observation
# retrieve most similar documents
#install.packages("text2vec")
#library(text2vec)
# function computes cosine similarity between query and all documents and returns N most similar
nearest_neighbors <- function(query, low_dim_space, N = 5, norm = "l2"){
cos_sim <- sim2(x = low_dim_space, y = low_dim_space[query, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # query is always the nearest neighbor hence dropped
}
# apply to document retrieval
nearest_neighbors(query = "2009-Obama", low_dim_space = inaugural_pca$x, N = 5, norm = "l2")
##### Exercise ######
# Question 1: Update the code so that it calculates the terms with the
#top loadings on PC2. What is the theme of this dimension?
# Question 2: Who are the 5 people Obama's inaugural address is most
#close to in 2013? What about Trump in 2017?
# Set up workspace
rm(list = ls())
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
# Loading packages
#install.packages("factoextra")
library(quanteda)
library(factoextra)
library(dplyr)
## 1 PCA
# 1.1  function in base R for PCA:
# see: http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/
?prcomp # uses the singular value decomposition approach: examines the covariances/correlations between individuals
# Remember to center your data! (default = TRUE) -- use scale() on your matrix beforehand, or the option in prcomp()
# And don't have any missing values!
# 1.2 Example
data("data_corpus_inaugural")
inaugural <- corpus_subset(data_corpus_inaugural, Year > "1900-01-01")
inaugural_dfm <- dfm(inaugural,
stem = T,
remove_punct = T,
remove = stopwords("english")
)
inaugural_mat <- convert(inaugural_dfm, to = "matrix") # convert to matrix
# run pca
inaugural_pca <- prcomp(inaugural_mat, center = TRUE, scale = TRUE)
# visualize eigenvalues (scree plot: shows percentage of variance explained by each dimension)
fviz_eig(inaugural_pca, addlabels = TRUE)
# Loadings for each variable: columns contain the eigenvectors
inaugural_pca$rotation[1:10, 1:5]
dim(inaugural_pca$rotation)
# Q: can we interpret the dimensions?
# top loadings on PC1
pc_loadings <- inaugural_pca$rotation
# what do we expect this correlation to be?
cor(pc_loadings[,1], pc_loadings[,2])  # these should be orthogonal
# token loadings
N <- 10
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
# plot top tokens according to absolute loading values
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
# Value of the rotated data: your "new", dimensionality reduced data
View(inaugural_pca$x)  # each observation
# retrieve most similar documents
# function computes cosine similarity between query and all documents and returns N most similar
nearest_neighbors <- function(query, low_dim_space, N = 5, norm = "l2"){
cos_sim <- sim2(x = low_dim_space, y = low_dim_space[query, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # query is always the nearest neighbor hence dropped
}
# apply to document retrieval
nearest_neighbors(query = "2009-Obama", low_dim_space = inaugural_pca$x, N = 5, norm = "l2")
##### Exercise ######
# Question 1: Update the code so that it calculates the terms with the
#top loadings on PC2. What is the theme of this dimension?
# Question 2: Who are the 5 people Obama's inaugural address is most
#close to in 2013? What about Trump in 2017?
rm(list = ls())
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
library(quanteda)
library(factoextra)
library(dplyr)
?prcomp # uses the singular value decomposition approach: examines the covariances/correlations between individuals
data("data_corpus_inaugural")
inaugural <- corpus_subset(data_corpus_inaugural, Year > "1900-01-01")
inaugural_dfm <- dfm(inaugural,
stem = T,
remove_punct = T,
remove = stopwords("english")
)
inaugural_mat <- convert(inaugural_dfm, to = "matrix") # convert to matrix
inaugural_pca <- prcomp(inaugural_mat, center = TRUE, scale = TRUE)
fviz_eig(inaugural_pca, addlabels = TRUE)
inaugural_pca$rotation[1:10, 1:5]
dim(inaugural_pca$rotation)
pc_loadings <- inaugural_pca$rotation
cor(pc_loadings[,1], pc_loadings[,2])  # these should be orthogonal
N <- 10
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,1])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
View(inaugural_pca$x)  # each observation
library(text2vec)
nearest_neighbors <- function(query, low_dim_space, N = 5, norm = "l2"){
cos_sim <- sim2(x = low_dim_space, y = low_dim_space[query, , drop = FALSE], method = "cosine", norm = norm)
nn <- cos_sim <- cos_sim[order(-cos_sim),]
return(names(nn)[2:(N + 1)])  # query is always the nearest neighbor hence dropped
}
nearest_neighbors(query = "2009-Obama", low_dim_space = inaugural_pca$x, N = 5, norm = "l2")
N <- 10
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,2])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
# plot top tokens according to absolute loading values
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
# Value of the rotated data: your "new", dimensionality reduced data
View(inaugural_pca$x)  # each observation
pc1_loading
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
pc1_loading
pc1_loading<-filter(pc1_loading$loading > 3 | pc1_loading$loading < -1 )
pc1_loading<-dplyr::filter(pc1_loading$loading > 3 | pc1_loading$loading < -1 )
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
pc1_loading<-dplyr::filter(pc1_loading, loading > 3 | loading < -1 )
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,2])) %>% arrange(-loading)
pc1_loading
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
pc1_loading
pc1_loading<-dplyr::filter(pc1_loading, loading > 3.2 | loading < -1 )
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,2])) %>% arrange(-loading)
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,2])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
pc1_loading<-dplyr::filter(pc1_loading, loading > 3.2 | loading < -1 )
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
pc1_loading <- tibble(token = rownames(pc_loadings), loading = as.vector(pc_loadings[,2])) %>% arrange(-loading)
pc1_loading$loading <- scale(pc1_loading$loading, center = TRUE)
pc1_loading <- rbind(top_n(pc1_loading, N, loading),top_n(pc1_loading, -N, loading))
pc1_loading <- transform(pc1_loading, token = factor(token, levels = unique(token)))
pc1_loading<-dplyr::filter(pc1_loading, loading > 3.2 | loading < -1 )
ggplot(pc1_loading, aes(token, loading)) +
geom_bar(stat = "identity", fill = ifelse(pc1_loading$loading <= 0, "grey20", "grey70")) +
coord_flip() +
xlab("Tokens") + ylab("Tokens with Top Loadings on PC1") +
scale_colour_grey(start = .3, end = .7) +
theme(panel.background = element_blank(),
axis.text.x = element_text(size=16),
axis.text.y = element_text(size=16),
axis.title.y = element_text(size=18, margin = margin(t = 0, r = 15, b = 0, l = 15)),
axis.title.x = element_text(size=18, margin = margin(t = 15, r = 0, b = 15, l = 0)),
legend.text=element_text(size=16),
legend.title=element_blank(),
legend.key=element_blank(),
legend.position = "top",
legend.spacing.x = unit(0.25, 'cm'),
plot.margin=unit(c(1,1,0,0),"cm"))
