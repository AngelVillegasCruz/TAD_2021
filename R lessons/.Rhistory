init_tiktokr()
trends <- get_trending(200)
user <- get_username("willsmith")
get_username("willsmith")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
readab <- textstat_readability(corpus_subset(data_corpus_inaugural, Year > 1980),
measure = "Flesch.Kincaid")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
textstat_simil(presDfm, presDfm["1985-Reagan",],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1900), stem = TRUE,
remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
presDistMat <- dist(as.matrix(dfm_weight(presDfm, "relFreq")))
presCluster <- hclust(presDistMat)
presCluster$labels <- docnames(presDfm)
dev.new()
plot(presCluster)
install.packages("quanteda")
install.packages("quanteda")
install.packages("quanteda")
require(quanteda)
txt <- c(sent1 = "This is an example of the summary method for character objects.",
sent2 = "The cat in the hat swung the bat.")
summary(txt)
txt
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
nsyllable(c("Superman.", "supercalifragilisticexpialidocious", "The cat in the hat."))
nscrabble(c("cat", "quixotry", "zoo"))
myDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980))
textstat_lexdiv(myDfm, "R")
presDfm <- dfm(data_corpus_inaugural, remove = stopwords("english"))
textstat_simil(presDfm, "1985-Reagan",  margin = "documents")
?textstat_simil
textstat_simil(presDfm, presDfm["1985-Reagan", ],  margin = "documents")
textstat_simil(presDfm, c("2009-Obama", "2013-Obama"), margin = "documents", method = "cosine")
summary(presDfm)
summary(corpus(data_char_ukimmig2010, notes = "Created as a demo."))
presDfm
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("glue")
install.packages("glue")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("rlang")
install.packages("rlang")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("processx")
install.packages("processx")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("fansi")
install.packages("fansi")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
install.packages("ps")
install.packages("ps")
devtools::install_github(
"jonathanbratt/RBERT",
build_vignettes = TRUE
)
library(RBERT)
BERT_PRETRAINED_DIR <- RBERT::download_BERT_checkpoint(
model = "bert_base_uncased"
)
text_to_process <- c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse.",
"An impulse is like a push.",
"Impulse is force times time.")
text_to_process2 <- list(c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse."),
c("An impulse is like a push.",
"Impulse is force times time."))
BERT_feats <- extract_features(
examples = text_to_process2,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
output_vector1 <- BERT_feats$output %>%
dplyr::filter(
sequence_index == 1,
token == "[CLS]",
layer_index == 12
) %>%
dplyr::select(dplyr::starts_with("V")) %>%
unlist()
output_vector1
install_tensorflow()
tensorflow::install_tensorflow(version = "1.13.1")
text_to_process <- c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse.",
"An impulse is like a push.",
"Impulse is force times time.")
text_to_process2 <- list(c("Impulse is equal to the change in momentum.",
"Changing momentum requires an impulse."),
c("An impulse is like a push.",
"Impulse is force times time."))
BERT_feats <- extract_features(
examples = text_to_process2,
ckpt_dir = BERT_PRETRAINED_DIR,
layer_indexes = 1:12
)
devtools::install_github("rstudio/reticulate")
library(quanteda)
library(gutenbergr)
library(dplyr)
library(tidytext)
CDworks <- gutenberg_works(author == "Dickens, Charles")
CD <- gutenberg_download(CDworks, meta_fields = "title")
dfm_CD <- CD %>%
unnest_tokens(word, text) %>%
count(title, word) %>%
cast_dfm(title, word, n)
corpus_CD <- CD %>%
unnest_tokens(word, text) %>%
group_by(title)
corpus_CD <- aggregate(word ~ title,data=corpus_CD, paste )
corpus_CD[1]
summary(dfm_CD)
prepare_dt <- function(book_id, removePunct = TRUE){
meta <- gutenberg_works(gutenberg_id  == book_id)
meta <- meta %>% mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
text <- gutenberg_download(book_id) %>%
select(text) %>%
filter(text!="") %>%
unlist() %>%
paste(., collapse = " ") %>%
str_replace_all(., "^ +| +$|( ) +", "\\1")
text <- gsub("`|'", "", text) # remove apostrophes
text <- gsub("[^[:alpha:]]", " ", text) # remove all non-alpha characters
output <- tibble(title = meta$title, author = meta$author, text = text)
}
joyce_twain <- lapply(c(4217, 74), prepare_dt, removePunct = TRUE) %>% do.call(rbind,.)
meta <- meta %>% dplyr::mutate(author = unlist(str_split(author, ","))[1] %>% tolower(.))
x<-filter(CD, gutenberg_id == 46)
xx<-pasteo(x$text)
xx<-paste0(x$text)
xx<-collapse(x$text)
xx<-paste(x$text)
?collapse
?paste()
xx<-paste(x$text,collapse = " ")
sum(
1606.25,
398.97,
519.50,
372.00,
375.50)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R_lessons/intro_R")
set.seed(1234)
# Check for these packages, install them if you don't have them
# install.packages("tidytext")
# install.packages("topicmodels")
# install.packages("stringi")
# install.packages("doParallel")
#install.packages("ldatuning")
library(ldatuning)
libraries <- c("ldatuning", "topicmodels", "ggplot2", "dplyr", "rjson", "quanteda", "lubridate", "parallel", "doParallel", "tidytext", "stringi", "tidyr")
lapply(libraries, require, character.only = TRUE)
## 1 Preprocessing
# Load data
blm_tweets <- read.csv("blm_samp.csv", stringsAsFactors = F)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R_lessons/")
setwd("C:/Users/kevin/Documents/GitHub/TAD2021/R_lessons/")
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
set.seed(1234)
blm_tweets <- read.csv("blm_samp.csv", stringsAsFactors = F)
blm_tweets <- read.csv("blm_samp.csv", stringsAsFactors = F)
blm_tweets$datetime <- as.POSIXct(strptime(blm_tweets$created_at, "%a %b %d %T %z %Y",tz = "GMT")) # full date/timestamp
blm_tweets$date <- mdy(paste(month(blm_tweets$datetime), day(blm_tweets$datetime), year(blm_tweets$datetime), sep = "-")) # date only
# Collapse tweets so we are looking at the total tweets at the day level
blm_tweets_sum <- blm_tweets %>% group_by(date) %>% summarise(text = paste(text, collapse = " "))
# Remove non ASCII characters
blm_tweets_sum$text <- stringi::stri_trans_general(blm_tweets_sum$text, "latin-ascii")
# Removes solitary letters
blm_tweets_sum$text <- gsub(" [A-z] ", " ", blm_tweets_sum$text)
# As always we begin with a DFM.
# Create DFM
blm_dfm <-dfm(blm_tweets_sum$text, stem = F, remove_punct = T, tolower = T, remove_twitter = T, remove_numbers = TRUE, remove = c(stopwords("english"), "http","https","rt", "t.co"))
# Topic models
## 2 Selecting K
# Identify an appropriate number of topics (FYI, this function takes a while)
k_optimize_blm <- FindTopicsNumber(
blm_dfm,
topics = seq(from = 2, to = 30, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 2017),
mc.cores = detectCores(), # to usa all cores available
verbose = TRUE
)
dev.new()
FindTopicsNumber_plot(k_optimize_blm)
# What does robustness mean here?
## 3 Visualizing Word weights
# Set number of topics
k <- 19
# Fit the topic model with the chosen k
system.time(
blm_tm <- LDA(blm_dfm, k = k, method = "Gibbs",  control = list(seed = 1234)))
##how does the model perform statistically?
blm_tm@loglikelihood
# gamma = posterior document distribution over topics
# what are the dimensions of gamma?
topics(blm_tm)
# Per topic per word proabilities matrix (beta)
get_terms(blm_tm, k = 5)
## 4 Visualizing topic trends over time
# Store the results of the mixture of documents over topics
doc_topics <- blm_tm@gamma
# Store the results of words over topics
#words_topics <- blm_tm@beta
# Transpose the data so that the days are columns
doc_topics <- t(doc_topics)
dim(doc_topics)
doc_topics[1:5,1:5]
# Arrange topics
# Find the top topic per column (day)
max <- apply(doc_topics, 2, which.max)
# Write a function that finds the second max
which.max2 <- function(x){
which(x == sort(x,partial=(k-1))[k-1])
}
max2 <- apply(doc_topics, 2, which.max2)
max2 <- sapply(max2, max)
# Coding police shooting events
victim <- c("Freddie Gray", "Sandra Bland")
shootings <- mdy(c("04/12/2015","7/13/2015"))
# Combine data
top2 <- data.frame(top_topic = max, second_topic = max2, date = ymd(blm_tweets_sum$date))
# Plot
blm_plot <- ggplot(top2, aes(x=date, y=top_topic, pch="First"))
blm_plot + geom_point(aes(x=date, y=second_topic, pch="Second") ) +theme_bw() +
ylab("Topic Number") + ggtitle("BLM-Related Tweets from 2014 to 2016 over Topics") + geom_point() + xlab(NULL) +
geom_vline(xintercept=as.numeric(shootings[1]), color = "blue", linetype=4) + # Freddie Gray (Topic)
geom_vline(xintercept=as.numeric(shootings[2]), color = "black", linetype=4)  + # Sandra Bland
scale_shape_manual(values=c(18, 1), name = "Topic Rank")
#### ## Modification for in-class assignment
###Sample just the first 5000 of the initial tweets. Re-run the topic selection algorithm. What does the best number of topics look like now?
#### Do you think that the number of topics is linear in the number of documents??
blm_plot + geom_point(aes(x=date, y=second_topic, pch="Second") ) +theme_bw() +
ylab("Topic Number") + ggtitle("BLM-Related Tweets from 2014 to 2016 over Topics") + geom_point() + xlab(NULL) +
geom_vline(xintercept=as.numeric(shootings[1]), color = "blue", linetype=4) + # Freddie Gray (Topic)
geom_vline(xintercept=as.numeric(shootings[2]), color = "black", linetype=4)  + # Sandra Bland
scale_shape_manual(values=c(18, 1), name = "Topic Rank")
rm(list = ls())
rm(list = ls())
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
set.seed(1234)
library(ldatuning)
libraries <- c("ldatuning", "topicmodels", "ggplot2", "dplyr", "rjson", "quanteda", "lubridate", "parallel", "doParallel", "tidytext", "stringi", "tidyr")
lapply(libraries, require, character.only = TRUE)
blm_tweets <- read.csv("blm_samp.csv", stringsAsFactors = F)
blm_tweets$datetime <- as.POSIXct(strptime(blm_tweets$created_at, "%a %b %d %T %z %Y",tz = "GMT")) # full date/timestamp
month(blm_tweets$datetime)
paste(month(blm_tweets$datetime), day(blm_tweets$datetime), year(blm_tweets$datetime), sep = "-")
blm_tweets$date <- mdy(paste(month(blm_tweets$datetime), day(blm_tweets$datetime), year(blm_tweets$datetime), sep = "-")) # date only
blm_tweets_sum <- blm_tweets %>% group_by(date) %>% summarise(text = paste(text, collapse = " "))
blm_tweets_sum$text <- stringi::stri_trans_general(blm_tweets_sum$text, "latin-ascii")
blm_tweets_sum$text <- gsub(" [A-z] ", " ", blm_tweets_sum$text)
blm_dfm <-dfm(blm_tweets_sum$text, stem = F, remove_punct = T, tolower = T, remove_twitter = T, remove_numbers = TRUE, remove = c(stopwords("english"), "http","https","rt", "t.co"))
k_optimize_blm <- FindTopicsNumber(
blm_dfm,
topics = seq(from = 2, to = 30, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 2017),
mc.cores = detectCores(), # to usa all cores available
verbose = TRUE
)
k <- 19
system.time(
blm_tm <- LDA(blm_dfm, k = k, method = "Gibbs",  control = list(seed = 1234)))
blm_tm@loglikelihood
topics(blm_tm)
get_terms(blm_tm, k = 5)
doc_topics <- blm_tm@gamma
doc_topics <- t(doc_topics)
dim(doc_topics)
doc_topics[1:5,1:5]
max <- apply(doc_topics, 2, which.max)
which.max2 <- function(x){
which(x == sort(x,partial=(k-1))[k-1])
}
max2 <- apply(doc_topics, 2, which.max2)
max2 <- sapply(max2, max)
victim <- c("Freddie Gray", "Sandra Bland")
shootings <- mdy(c("04/12/2015","7/13/2015"))
top2 <- data.frame(top_topic = max, second_topic = max2, date = ymd(blm_tweets_sum$date))
blm_plot <- ggplot(top2, aes(x=date, y=top_topic, pch="First"))
blm_plot + geom_point(aes(x=date, y=second_topic, pch="Second") ) +theme_bw() +
ylab("Topic Number") + ggtitle("BLM-Related Tweets from 2014 to 2016 over Topics") + geom_point() + xlab(NULL) +
geom_vline(xintercept=as.numeric(shootings[1]), color = "blue", linetype=4) + # Freddie Gray (Topic)
geom_vline(xintercept=as.numeric(shootings[2]), color = "black", linetype=4)  + # Sandra Bland
scale_shape_manual(values=c(18, 1), name = "Topic Rank")
blm_tweets_2<-blm_tweets[1:5000,]
NYTIMES_KEY <- ("992a3e75-d08d-4ada-8f57-f93c63287589")
term <- "facebook"
begin_date <- "20140618"
end_date <- "20140706"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
library(jsonlite)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
NYTIMES_KEY <- ("0ZIN5m5qCvEzfwKFrclUOI2plxpAkoDw")
term <- "facebook"
begin_date <- "20140618"
end_date <- "20140706"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
pages_2014 <- vector("list",length=maxPages)
for(i in 0:maxPages){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2014[[i+1]] <- nytSearch
Sys.sleep(5) #I was getting errors more often when I waited only 1 second between calls. 5 seconds seems to work better.
}
facebook_2014_articles <- rbind_pages(pages_2014)
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
baseurl
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
begin_date <- "20200618"
end_date <- "20200706"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
pages_2020 <- vector("list",length=maxPages)
for(i in 0:maxPages){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2020[[i+1]] <- nytSearch
Sys.sleep(5) #I was getting errors more often when I waited only 1 second between calls. 5 seconds seems to work better.
}
for(i in 0:2){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2020[[i+1]] <- nytSearch
Sys.sleep(5) #I was getting errors more often when I waited only 1 second between calls. 5 seconds seems to work better.
}
facebook_2020_articles <- rbind_pages(pages_2020)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
save(facebook_2020_articles,file="facebook_2020_articles.Rdata")
for(i in 0:10){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2020[[i+1]] <- nytSearch
Sys.sleep(5) #I was getting errors more often when I waited only 1 second between calls. 5 seconds seems to work better.
}
facebook_2020_articles <- rbind_pages(pages_2020)
save(facebook_2020_articles,file="facebook_2020_articles.Rdata")
term <- "facebook"
begin_date <- "20210101"
end_date <- "20210401"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
pages_2021 <- vector("list",length=maxPages)
for(i in 0:10){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2021[[i+1]] <- nytSearch
Sys.sleep(5)
}
facebook_2021_articles <- rbind_pages(pages_2021)
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
NYTIMES_KEY <- ("0ZIN5m5qCvEzfwKFrclUOI2plxpAkoDw")
term <- "facebook"
begin_date <- "20200101"
end_date <- "20200401"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
pages_2020 <- vector("list",length=maxPages)
for(i in 0:2){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2020[[i+1]] <- nytSearch
Sys.sleep(10) #I was getting errors more often when I waited only 1 second between calls. 5 seconds seems to work better.
}
facebook_2020_articles <- rbind_pages(pages_2020)
save(facebook_2020_articles,file="facebook_2020_articles.Rdata")
term <- "facebook"
begin_date <- "20210101"
end_date <- "20210401"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
pages_2021 <- vector("list",length=maxPages)
for(i in 0:2){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2021[[i+1]] <- nytSearch
Sys.sleep(10)
}
facebook_2021_articles <- rbind_pages(pages_2021)
save(facebook_2021_articles,file="facebook_2021_articles.Rdata")
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
pages_2020 <- vector("list",length=3)
for(i in 0:2){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2020[[i+1]] <- nytSearch
Sys.sleep(10) #I was getting errors more often when I waited only 1 second between calls. 5 seconds seems to work better.
}
facebook_2020_articles <- rbind_pages(pages_2020)
pages_2020 <- vector("list",length=5)
for(i in 0:4){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2020[[i+1]] <- nytSearch
Sys.sleep(10) #I was getting errors more often when I waited only 1 second between calls. 5 seconds seems to work better.
}
facebook_2020_articles <- rbind_pages(pages_2020)
term <- "facebook"
begin_date <- "20210101"
end_date <- "20210401"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
pages_2021 <- vector("list",length=5)
for(i in 0:5){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2021[[i+1]] <- nytSearch
Sys.sleep(10)
}
facebook_2021_articles <- rbind_pages(pages_2021)
View(facebook_2020_articles)
View(facebook_2020_articles)
library(jsonlite)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
##code adapted from Heather Geiger
setwd("C:/Users/kevin/Documents/GitHub/TAD_2021/R lessons/")
NYTIMES_KEY <- ("ehIDV5b7qlZAd0A2IiLJ88XU1JqFN6BC")
term <- "facebook"
begin_date <- "20200101"
end_date <- "20200401"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
baseurl
initialQuery <- fromJSON(baseurl)
pages_2020 <- vector("list",length=5)
for(i in 0:4){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2020[[i+1]] <- nytSearch
Sys.sleep(10) #I was getting errors more often when I waited only 1 second between calls. 5 seconds seems to work better.
}
facebook_2020_articles <- rbind_pages(pages_2020)
facebook_2020_articles <- rbind_pages(pages_2020)
View(facebook_2020_articles)
term <- "facebook"
begin_date <- "20210101"
end_date <- "20210401"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
"&begin_date=",begin_date,"&end_date=",end_date,
"&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
initialQuery <- fromJSON(baseurl)
pages_2021 <- vector("list",length=5)
for(i in 0:5){
nytSearch <- fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame()
pages_2021[[i+1]] <- nytSearch
Sys.sleep(10)
}
facebook_2021_articles <- rbind_pages(pages_2021)
View(facebook_2021_articles)
table(facebook_2021_articles$response.docs.section_name)
table(facebook_2020_articles$response.docs.section_name)
x<-textstat_readability(facebook_2021_articles$response.docs.lead_paragraph)
xx<-textstat_readability(facebook_2020_articles$response.docs.lead_paragraph)
x
mean(x$Flesch)
mean(xx$Flesch)
